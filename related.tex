\section{Related Work}
\label{sec:related}

At first blush, this problem seems reminiscent of the problem of generating
tests for programs modeled as control-flow graphs. 
Most notably, techniques such as \textit{concolic} testing
attempt to identify test data that would force program execution through
different paths~\cite{dart, concolic}. Other approaches are based on model
checking, with the goal of creating test inputs to reach specific program
states. However, the problem of test generation from business rules is
different. Business rules do not describe the implementation of a system;
rather, they describe only a model and many of the concerns that arise when
dealing with control flow in real code are not pertinent.

\paragraph*{Model Based Testing} Business rules can viewed as a model of a system
and our approach then becomes a variant of model-based test
generation~\cite{utting2012}. Typical modeling languages used in model-based
testing include UML sequence diagrams~\cite{nayak2009}, modeling
languages such as Systems Modeling Language~\cite{friedenthal2011}, and
finite state machine notations such as UML state charts~\cite{offhut99}. While
the business rules as presented in this paper could be expressed in these
notations, it would be cumbersome and require some degree of non-trivial
encoding on part of users, making it unattractive to a non-programmer such as a
business analyst. Our business rule language is designed to be user friendly,
where syntax is close to prose, thereby making it more accessible to
non-programmers.

%Sriganesh and Ramanathan~\cite{sriganesh2012} extracts business rules
%from systems described in the Business Process Model and Notation
%language, a graphical notation used to describe business
%processes. The rule output is in a format similar to our formalism and
%our algorithm could conceivably be applied to generate test sequences
%from those rules.

The work on combinatorial test optimization~\cite{segall2012} also requires a model 
of an application as a starting point, but that model is typically not rich
enough to allow generation of test scenarios; the model does not have an operation-based
view of the application. However, test optimization 
can be carried out in a post pass over the tests generated by our technique.

\paragraph*{AI Planning}Previous work has applied AI planning to software
testing~\cite{Scheetz99ai,Howe97testcase}. The sequencing of operations done by
our algorithm is similar to AI planning~\cite{Weld94} where actions with pre-
and postconditions are sequenced by a planner algorithm using forward or
backward chaining. The algorithms used by AI planners are similar to \exhaust{}
(see Section~\ref{sec:eval}), which proved to be insufficient for our
benchmarks. Part of an AI planning problem is the initial state of the program,
including what objects exist and what conditions can be assumed. Our technique
does not need such information---the types of objects that exist in the system
are defined in the model along with operations that create them.
%A test sequence will construct any object needed to
%cover a given rule part.

Paradkar et al.~\cite{conf/icws/ParadkarSWJOSL07} present a system for testing
web services specified in OWL-S~\cite{owls}. The system is backed by an AI
planner. OWL-S represents operations in a manner similar to our language with
pre- and postconditions. However, each operation has only one associated pre- and
postcondition pair, making it more akin to a rule part in our language. If one
were to translate a model in our business rule language into OWL-S, each rule
would be translated into a standalone operation. Such a translation could
potentially increase the size of the search space and lead to spurious sequences
being constructed. The focus of their work is on conformance testing, that is,
to test whether a given implementation conforms to the specification. In our
approach, business rules are the specification and the aim is to generate
sequences to cover all rules. %% Their
%% paper does not mention the size of the models the technique was evaluated on nor
%% the size of the generated test sequences.

\paragraph*{Testing Object-Oriented Programs} Our work is also related to
techniques that focus on test generation for object-oriented
programs~\cite{tillmann2008,thummalapenta2011, tonella:etoc}.
%At a high level,
%these techniques also focus on inferring operation sequences 
%with a goal of generating different object states for receiver
%or arguments of method under test so as to achieve high code coverage.
%The main novelty of our current technique 
%Among these techniques, Symstra~\cite{xie:symstra} uses
%bounded-exhaustive technique, Randoop~\cite{pacheco2007} and JCrasher~\cite{csallner:jcrasher} 
%select methods randomly to compose sequences, and Tonella~\cite{tonella:etoc} and McMinn et al.~\cite{phil:evo}
%use evolutionary approaches. 
Among these, our work is closely related to Seeker~\cite{thummalapenta2011},
which also attempts to compose sequences incrementally based on branches that
are not yet covered. The major contribution of Seeker is to infer candidate
operations especially when those branches include private fields, since private
fields cannot be modified directly. For covering business rules, this is not an
issue---our model does not have the notion of public and private attributes. We
cannot compare \tool{} with Seeker because they take different inputs: \tool{}
requires business rules in our modeling language, whereas Seeker requires C\#
programs.
%% However, we believe that Seeker can greatly benefit with
%% our new algorithm of leveraging unsatisfied core as a guidance for suggesting
%% candidate operations.

%Finally, our previous work \wateg{}~\cite{Thummalapenta:2013} also accepts rules 
%and generates tests by crawling an application's GUI. However, the rules handled in our previous
%work are primarily related to application behaviors such as access control
%or navigational properties. Thus, those rules are expressed 
%using GUI elements such as links, text boxes, or buttons. In contrast,
%in our current work, our rules express business logic and the goal is to
%generate sufficient test data to exercise those rules. 
